{"metadata":{"colab":{"authorship_tag":"ABX9TyMRpOGQjpoxurAw6GSGiv5U","include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7195082,"sourceType":"datasetVersion","datasetId":4161034},{"sourceId":7195109,"sourceType":"datasetVersion","datasetId":4161054}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **LoreWeaver Project**\n\n![LoreWeaver](https://private-user-images.githubusercontent.com/81964452/284000208-5644db74-67d0-49f1-8362-3661c99361de.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MDQ0ODcxNjAsIm5iZiI6MTcwNDQ4Njg2MCwicGF0aCI6Ii84MTk2NDQ1Mi8yODQwMDAyMDgtNTY0NGRiNzQtNjdkMC00OWYxLTgzNjItMzY2MWM5OTM2MWRlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNDAxMDUlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjQwMTA1VDIwMzQyMFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWUxMTBjNDgxNDk2ZThhNzAyNzM3OTY1YjVlMGZjN2ZkMzZlZjVjNWNlZGY3OGVhODVmZjMzZTExYTE1YmYyZmQmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.-eOSvWJFhTre9jtdwldA_w2iguNqr1KEaWJhwdlTqBA)","metadata":{"id":"lJ4soaSMhgUo"}},{"cell_type":"markdown","source":"## **How to fine-tune Mistral**\n\nIn this section, we will fine-tune a 7B parameter Mistral model on a A100 GPU with high RAM using Jupyter Notebook. Note that a A100 has 80 GB of VRAM, which is enough to store Mistral 7B’s  weights (7b × 2 bytes = 14 GB in FP16). In addition, we need to consider the overhead due to optimizer states, gradients, and forward activations (see this excellent article for more information). This means that a full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like QLoRA.\n\nTo drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here. The good thing is that we can leverage the Hugging Face ecosystem with the transformers, accelerate, peft, trl, and bitsandbytes libraries. First, we install and load these libraries.","metadata":{"id":"9sFY3weAiG0X"}},{"cell_type":"code","source":"!pip install -U trl \n!pip install -U accelerate \n!pip install -U peft \n!pip install -U bitsandbytes \n!pip install -U transformers \n!pip install -U trl \n!pip install -U scipy\n!pip install -U datasets==2.16.0","metadata":{"id":"JJANcmlUhpk-","outputId":"7d51f366-8025-4980-f791-cb50ba5c6f70","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os,torch, logging\nfrom tqdm.notebook import tqdm\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nfrom trl import SFTTrainer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HuggingFace\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> **First, we want to load a mistral-7B model and train it on the atlas storyteller (5k rows), which will produce our fine-tuned model mistral-7B-LoreWeaver.**\n\nQLoRA will use a rank of 64 with a scaling parameter of 16 (see this article for more information about LoRA parameters). We’ll load the Llama 2 model directly in 4-bit precision using the NF4 type and train it for one epoch. To get more information about the other parameters, check the TrainingArguments, PeftModel, and SFTTrainer documentation.","metadata":{"id":"dy79lphLjFZo"}},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\nmodel_name = \"mistralai/Mistral-7B-v0.1\"\n\n# The instruction dataset to use\ndataset_name = \"AtlasUnified/atlas-storyteller\"\n\n# Fine-tuned model name\nnew_model = \"Mistral-7B-LoreWeaver\"","metadata":{"id":"WZ9KcGFqjVj-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### QLoRA parameters","metadata":{}},{"cell_type":"code","source":"################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 16\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 32\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.05","metadata":{"id":"7Oxu2QXTjb38","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### bitsandbytes parameters","metadata":{}},{"cell_type":"code","source":"################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = torch.bfloat16\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False","metadata":{"id":"tdXSjZOLjf_-","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n## Global Configs","metadata":{}},{"cell_type":"markdown","source":"### TrainingArguments parameters","metadata":{}},{"cell_type":"code","source":"################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = True\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_8bit\"\n\n# Learning rate schedule (constant a bit better than cosine)\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 25\n\n# Log every X updates steps\nlogging_steps = 25","metadata":{"id":"GOQhnX68h_6H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SFT parameters","metadata":{}},{"cell_type":"code","source":"################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = 1024\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False\n\n# Load the entire model on the GPU 0\ndevice_map = \"auto\"","metadata":{"id":"vlaRC-1ejoAg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **We can now start a fine-tuning process**\n\n1. Load the pre-processed dataset.\n2. Configure bitsandbytes for 4-bit quantization\n3. Load mistral model in 4-bit precision on a GPU with tokenizer\n4. Load configurations for QLoRA, regular training params using SFTTrainer","metadata":{"id":"Yj2SRFXfjy19"}},{"cell_type":"code","source":"# Step 1: Load the Atlas Storyteller Dataset\ndataset = load_dataset(dataset_name, split=\"train\", data_files=\"*.jsonl\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: configure bitsandbytes for 4-bit quantization\n\n# Load tokenizer and model with QLoRA configuration\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)","metadata":{"id":"kJN8qgbpk2Qq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 3: Load Mistral model in 4-bit precision on a GPU with tokenizer\n\n# Load Mistral base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    use_flash_attention_2=True,\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    device_map=device_map,\n    trust_remote_code=True\n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load Mistral tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"id":"V7BgWp2BlM8N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 4: Load configurations for QLoRA\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)","metadata":{"id":"kusnpXbUlkqi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Start training the model with given dataset**","metadata":{"id":"dlsWiGROl6Eo"}},{"cell_type":"code","source":"# Uncomment when NEEDED\n#os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:<14000>\"\n#torch.cuda.empty_cache()\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"Story\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)\n\n# Train model\ntrainer.train()\n\n# Save trained model\ntrainer.model.save_pretrained(new_model)","metadata":{"id":"Z1gEgaGhmL_k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> The training can be very long, depending on the size of your dataset. Here, it took less an hour and a half on a A100 GPU.","metadata":{"id":"ag7Goqb1maby"}},{"cell_type":"markdown","source":"It is very coherent for a model with only 7 billion parameters. You can play with it and ask harder questions from evaluation datasets like BigBench-Hard. \n\nHow can we store our new Mistral-7B-LoreWeaver model now? We need to merge the weights from LoRA with the base model. Unfortunately, as far as I know, there is no straightforward way to do it: we need to reload the base model in FP16 precision and use the peft library to merge everything. Alas, it also creates a problem with the VRAM (despite emptying it), so I recommend restarting the notebook, re-executing the three first cells, and then executing the next one.","metadata":{"id":"iqcSsD3EnTvc"}},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\nadapter_peft_model_path = \"Reverb/Mistral-7B-LoreWeaver\"\noutput_dir = \"/kaggle/working/\"\ndevice = \"auto\"  \npush_to_hub = True \n\nif torch.cuda.is_available():\n    device = \"cuda:0\"\nelse:\n    device = \"cpu\"\n\ndevice_arg = {'device_map': 'auto'} if device == 'auto' else {'device_map': {\"\": device}}\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ntry:\n    logger.info(f\"Loading base model: {base_model_name_or_path}\")\n    with tqdm(total=1, desc=\"Loading base model\") as pbar:\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_name_or_path,\n            return_dict=True,\n            torch_dtype=torch.float16,\n            **device_arg\n        )\n        pbar.update(1)\n\n    offload_dir = \"./peft_offloads\"\n    if not os.path.exists(offload_dir):\n        os.makedirs(offload_dir)\n\n    logger.info(f\"Loading Peft: {adapter_peft_model_path}\")\n    with tqdm(total=1, desc=\"Loading Peft model\") as pbar:\n        model = PeftModel.from_pretrained(base_model, adapter_peft_model_path, offload_dir=offload_dir)\n        pbar.update(1)\n\n    logger.info(\"Running merge_and_unload\")\n    with tqdm(total=1, desc=\"Merge and Unload\") as pbar:\n        model = model.merge_and_unload()\n        pbar.update(1)\n\n    tokenizer = AutoTokenizer.from_pretrained(base_model_name_or_path)\n\n    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n    output_filename = f\"merged_model_{timestamp}.bin\"\n    output_filepath = os.path.join(output_dir, output_filename)\n    model.save_pretrained(output_filepath)\n    tokenizer.save_pretrained(output_filepath.replace('models', 'tokenizers'))\n    logger.info(f\"Model saved to {output_filepath}\")\n\nexcept Exception as e:\n    logger.exception(\"An error occurred:\")\n    raise","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our weights are merged and we reloaded the tokenizer. We can now push everything to the Hugging Face Hub to save our model.","metadata":{"id":"z__SI47mncdY"}}]}